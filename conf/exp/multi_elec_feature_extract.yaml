runner:
    lr: 5e-4
    optim: AdamW_finetune
    train_batch_size: 128
    valid_batch_size: 128
    shuffle: False
    multi_gpu: False
    device: cuda
    total_steps: 2000 
    num_workers: 16
    log_step: 100
    checkpoint_step: 100
    grad_clip: 1.0
    output_tb: True
    #start_from_ckpt: ???
    scheduler:
        name: ramp_up
        total_steps: ${exp.runner.total_steps}
        warmup: 0.025
        gamma: 0.95
